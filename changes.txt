changed provider configuration to 0.13
changed tags to tag in resources "aws_autoscaling_group"
moved all files into modules
moved user-data into ./user-data

Docs
DNS

ssh -i id_rsa ec2-user@<bastion-public-ip>
add to ~/.ssh/config
ServerAliveInterval 50
user for ssh to ec2 is ubuntu

Improvements
Enable session manager and remove bastion
Automate manual steps
Change the name of the internal and external elbs which are currently

Name                               DNS Name
master - internal-master.eu-west-2.elb.amazonaws.com
master - master.eu-west-2.elb.amazonaws.com

Questions
How is this set
echo $HOSTEDZONE_NAME && echo $AWS_DEFAULT_REGION

cfssl gencert \
-ca=ca.pem \
-ca-key=ca-key.pem \
-config=ca-config.json \
-hostname=10.32.0.1,${ETCD1_INTERNAL},${MASTER1_INTERNAL},${WORKER1_INTERNAL},\
etcd1.internal.${HOSTEDZONE_NAME},master1.internal.${HOSTEDZONE_NAME},worker1.internal.${HOSTEDZONE_NAME},\
${MASTER_ELB_PRIVATE},${MASTER_ELB_PUBLIC},127.0.0.1,kubernetes.default \
-profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes

from bastion this works
ssh -i k8 ubuntu@ip.eu-west-1.compute.internal
scp -i k8 ca.pem ubuntu@ip.eu-west-1.compute.internal:~
scp -i k8 ca.pem ubuntu@ip:~/

fails
ssh -i k8 ubuntu@etcd1.internal.k8-the-hard-way.com
scp -i k8 ca.pem ubuntu@etcd1.internal.k8-the-hard-way.com:~